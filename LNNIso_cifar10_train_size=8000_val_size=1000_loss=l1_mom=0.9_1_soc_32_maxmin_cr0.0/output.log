Namespace(activation='maxmin', batch_size=128, block_size=1, conv_layer='soc', data_dir='./cifar-data', dataset='cifar10', epochs=200, epsilon=36, eval_only=False, gamma=0.0, init_channels=32, lln=False, loss='l1', loss_scale='1.0', lr_max=0.01, lr_min=0.0, momentum=0.9, opt_level='O2', out_dir='LNNIso_cifar10_train_size=8000_val_size=1000_loss=l1_mom=0.9_1_soc_32_maxmin_cr0.0', sample_size=8000, seed=0, val_size=1000, weight_decay=0.0005)
Epoch 	 Seconds 	 LR 	 Train Loss 	 Val Loss 	 
0 	 12.3 	 0.0100 	 -0.0242 	 -0.2367
1 	 10.5 	 0.0100 	 -0.1787 	 -0.6584
2 	 9.6 	 0.0100 	 -0.2464 	 -0.2300
3 	 9.9 	 0.0100 	 -0.2997 	 -0.1598
4 	 10.5 	 0.0100 	 -0.3086 	 -0.6701
5 	 9.6 	 0.0100 	 -0.4157 	 -0.5894
6 	 9.9 	 0.0100 	 -0.4521 	 -0.6628
7 	 9.9 	 0.0100 	 -0.5121 	 -0.6282
8 	 9.6 	 0.0100 	 -0.5549 	 -0.1386
9 	 9.9 	 0.0100 	 -0.5741 	 -0.5815
10 	 9.7 	 0.0100 	 -0.6117 	 -0.1269
11 	 9.9 	 0.0100 	 -0.6665 	 -0.3843
12 	 9.9 	 0.0100 	 -0.6849 	 -0.1866
13 	 9.6 	 0.0100 	 -0.7356 	 -0.0727
14 	 9.9 	 0.0100 	 -0.7926 	 -0.0575
15 	 9.8 	 0.0100 	 -0.8516 	 -0.0911
16 	 10.3 	 0.0100 	 -0.8348 	 -0.8752
17 	 9.8 	 0.0100 	 -0.9123 	 -0.1716
18 	 9.7 	 0.0100 	 -0.9879 	 -0.3731
19 	 9.9 	 0.0100 	 -1.0181 	 -0.1835
20 	 9.9 	 0.0100 	 -1.0608 	 -0.1378
21 	 9.7 	 0.0100 	 -1.1838 	 -0.1668
22 	 10.3 	 0.0100 	 -1.1954 	 -0.3837
23 	 10.3 	 0.0100 	 -1.3045 	 -0.8262
24 	 10.1 	 0.0100 	 -1.2960 	 -0.1202
25 	 10.4 	 0.0100 	 -1.3629 	 -0.8032
26 	 10.3 	 0.0100 	 -1.4297 	 -0.7012
27 	 10.1 	 0.0100 	 -1.5086 	 -0.2942
28 	 10.3 	 0.0100 	 -1.5720 	 -0.4271
29 	 10.2 	 0.0100 	 -1.6049 	 -0.4782
30 	 9.9 	 0.0100 	 -1.5334 	 -0.5247
31 	 9.8 	 0.0100 	 -1.7557 	 -0.5307
32 	 9.7 	 0.0100 	 -1.8549 	 -0.0219
33 	 9.8 	 0.0100 	 -1.9141 	 -0.1240
34 	 9.8 	 0.0100 	 -1.9480 	 -0.5345
35 	 9.6 	 0.0100 	 -2.0243 	 -0.4639
36 	 9.8 	 0.0100 	 -2.0594 	 -0.4306
37 	 9.6 	 0.0100 	 -2.1452 	 -0.1292
38 	 9.9 	 0.0100 	 -2.2107 	 -0.1122
39 	 9.9 	 0.0100 	 -2.2893 	 -0.0302
40 	 9.6 	 0.0100 	 -2.3404 	 -0.2722
41 	 9.9 	 0.0100 	 -2.3785 	 -0.1037
42 	 9.9 	 0.0100 	 -2.4663 	 -0.4377
43 	 9.7 	 0.0100 	 -2.5360 	 -0.0601
44 	 9.9 	 0.0100 	 -2.5909 	 -0.1824
45 	 9.6 	 0.0100 	 -2.6477 	 -0.0757
46 	 9.9 	 0.0100 	 -2.6727 	 -0.3929
47 	 9.9 	 0.0100 	 -2.7448 	 -0.0710
48 	 9.6 	 0.0100 	 -2.7816 	 -0.3972
49 	 9.9 	 0.0100 	 -2.8352 	 -0.1450
50 	 9.8 	 0.0100 	 -2.9053 	 -0.6163
51 	 9.6 	 0.0100 	 -2.9659 	 -0.0510
52 	 9.9 	 0.0100 	 -3.0355 	 -0.0523
53 	 9.9 	 0.0100 	 -3.0749 	 -0.0567
54 	 9.6 	 0.0100 	 -3.1061 	 -0.2996
55 	 9.9 	 0.0100 	 -3.1349 	 -0.1089
56 	 9.6 	 0.0100 	 -3.1765 	 -0.3204
57 	 9.9 	 0.0100 	 -3.2612 	 -0.6514
