Namespace(activation='maxmin', batch_size=128, block_size=1, conv_layer='soc', data_dir='./cifar-data', dataset='cifar10', epochs=200, epsilon=36, eval_only=False, gamma=0.0, init_channels=32, lln=False, loss='l1', loss_scale='1.0', lr_max=0.01, lr_min=0.0, momentum=0.9, opt_level='O2', out_dir='LNNIso_cifar10_train_size=15000_val_size=1000_loss=l1_mom=0.9_1_soc_32_maxmin_cr0.0', sample_size=15000, seed=0, val_size=1000, weight_decay=0.0005)
Epoch 	 Seconds 	 LR 	 Train Loss 	 Val Loss 	 
0 	 19.4 	 0.0100 	 0.0152 	 -0.2147
1 	 18.0 	 0.0100 	 -0.0941 	 -0.4704
2 	 17.0 	 0.0100 	 -0.1332 	 -0.0548
3 	 17.0 	 0.0100 	 -0.1805 	 -0.2417
4 	 16.9 	 0.0100 	 -0.2151 	 -0.1400
5 	 17.2 	 0.0100 	 -0.2175 	 -0.1594
6 	 17.0 	 0.0100 	 -0.2692 	 -0.0960
7 	 16.9 	 0.0100 	 -0.2703 	 -0.0231
8 	 16.9 	 0.0100 	 -0.3412 	 -0.1357
9 	 18.2 	 0.0100 	 -0.3807 	 -0.7618
10 	 17.4 	 0.0100 	 -0.3888 	 -0.3292
11 	 17.7 	 0.0100 	 -0.4348 	 -0.4930
12 	 18.6 	 0.0100 	 -0.4682 	 -1.1779
13 	 17.5 	 0.0100 	 -0.5030 	 -0.2666
14 	 17.4 	 0.0100 	 -0.5432 	 -0.6974
15 	 17.4 	 0.0100 	 -0.6095 	 -0.7711
16 	 17.7 	 0.0100 	 -0.6207 	 -0.3895
17 	 17.5 	 0.0100 	 -0.6721 	 -0.1978
18 	 17.6 	 0.0100 	 -0.7232 	 -0.3291
19 	 17.0 	 0.0100 	 -0.7666 	 -0.1733
20 	 16.9 	 0.0100 	 -0.8301 	 -0.1165
21 	 17.0 	 0.0100 	 -0.8916 	 -0.4747
22 	 17.2 	 0.0100 	 -0.9051 	 -0.3312
23 	 16.9 	 0.0100 	 -0.9642 	 -0.3188
24 	 16.9 	 0.0100 	 -1.0079 	 -0.3125
25 	 16.9 	 0.0100 	 -1.0647 	 -0.0121
26 	 17.0 	 0.0100 	 -1.1013 	 -0.7102
27 	 17.2 	 0.0100 	 -1.1603 	 -0.1983
28 	 16.9 	 0.0100 	 -1.2147 	 -0.3899
29 	 16.9 	 0.0100 	 -1.2707 	 -0.0923
30 	 16.9 	 0.0100 	 -1.3645 	 -0.0039
31 	 16.9 	 0.0100 	 -1.3565 	 -0.0254
32 	 16.9 	 0.0100 	 -1.4740 	 -0.0120
33 	 17.2 	 0.0100 	 -1.5309 	 -0.0512
34 	 16.9 	 0.0100 	 -1.5129 	 -0.0898
35 	 16.9 	 0.0100 	 -1.6179 	 -0.4890
36 	 17.0 	 0.0100 	 -1.6918 	 -0.6635
37 	 16.9 	 0.0100 	 -1.7153 	 -0.4068
38 	 17.1 	 0.0100 	 -1.8063 	 -0.0890
39 	 16.9 	 0.0100 	 -1.8203 	 -0.3524
40 	 16.9 	 0.0100 	 -1.8662 	 -0.3415
41 	 16.9 	 0.0100 	 -1.9415 	 -0.2863
42 	 17.0 	 0.0100 	 -1.9536 	 -0.5094
43 	 16.9 	 0.0100 	 -2.0136 	 -0.1763
44 	 17.2 	 0.0100 	 -2.0558 	 -0.0812
45 	 16.9 	 0.0100 	 -2.1031 	 -0.3069
46 	 16.9 	 0.0100 	 -2.1801 	 -0.0606
47 	 16.9 	 0.0100 	 -2.1877 	 -0.1619
48 	 17.0 	 0.0100 	 -2.2774 	 -0.3190
49 	 16.9 	 0.0100 	 -2.3037 	 -0.3174
50 	 17.2 	 0.0100 	 -2.3107 	 -0.2481
51 	 16.9 	 0.0100 	 -2.4197 	 -0.1351
52 	 16.9 	 0.0100 	 -2.4828 	 -0.0379
53 	 16.9 	 0.0100 	 -2.5248 	 -0.1042
54 	 17.0 	 0.0100 	 -2.5210 	 -0.2697
55 	 17.2 	 0.0100 	 -2.6031 	 -0.1122
56 	 16.9 	 0.0100 	 -2.6215 	 -0.1474
57 	 17.0 	 0.0100 	 -2.6644 	 -0.1026
58 	 17.0 	 0.0100 	 -2.7424 	 -0.3381
59 	 17.0 	 0.0100 	 -2.7253 	 -0.0704
60 	 16.9 	 0.0100 	 -2.8139 	 -0.1669
61 	 17.1 	 0.0100 	 -2.8255 	 -0.3670
62 	 16.9 	 0.0100 	 -2.8764 	 -0.0540
63 	 16.9 	 0.0100 	 -2.9036 	 -0.2790
64 	 16.9 	 0.0100 	 -2.9567 	 -0.5215
65 	 16.9 	 0.0100 	 -2.9920 	 -0.0390
66 	 17.2 	 0.0100 	 -3.0137 	 -0.1678
67 	 16.9 	 0.0100 	 -3.0654 	 -0.5881
68 	 16.9 	 0.0100 	 -3.0978 	 -0.4652
69 	 16.9 	 0.0100 	 -3.1347 	 -0.4131
70 	 16.9 	 0.0100 	 -3.1954 	 -0.1689
71 	 16.9 	 0.0100 	 -3.1919 	 -0.4168
72 	 17.2 	 0.0100 	 -3.2424 	 -0.2954
73 	 16.9 	 0.0100 	 -3.3095 	 -0.0446
74 	 16.9 	 0.0100 	 -3.2845 	 -0.0409
75 	 16.9 	 0.0100 	 -3.3630 	 -0.0538
76 	 17.0 	 0.0100 	 -3.3873 	 -0.0468
77 	 17.2 	 0.0100 	 -3.4004 	 -0.0703
78 	 16.9 	 0.0100 	 -3.4430 	 -0.0229
79 	 16.9 	 0.0100 	 -3.4854 	 -0.0884
80 	 17.1 	 0.0100 	 -3.5025 	 -0.6346
81 	 17.1 	 0.0100 	 -3.5192 	 -0.0501
82 	 17.1 	 0.0100 	 -3.5178 	 -0.3760
83 	 17.3 	 0.0100 	 -3.5672 	 -0.4443
84 	 17.1 	 0.0100 	 -3.5990 	 -0.0880
85 	 17.0 	 0.0100 	 -3.6513 	 -0.1247
86 	 17.1 	 0.0100 	 -3.6661 	 -0.2104
87 	 17.0 	 0.0100 	 -3.6886 	 -0.1653
88 	 17.3 	 0.0100 	 -3.6947 	 -0.3591
89 	 17.1 	 0.0100 	 -3.7343 	 -0.1869
90 	 17.0 	 0.0100 	 -3.7557 	 -0.1425
91 	 17.1 	 0.0100 	 -3.7847 	 -0.2256
92 	 17.1 	 0.0100 	 -3.7935 	 -0.2344
93 	 17.1 	 0.0100 	 -3.7866 	 -0.1758
94 	 17.4 	 0.0100 	 -3.8502 	 -0.1144
95 	 17.0 	 0.0100 	 -3.8727 	 -0.1342
96 	 17.1 	 0.0100 	 -3.8904 	 -0.1178
97 	 17.3 	 0.0100 	 -3.9402 	 -0.4434
98 	 17.3 	 0.0100 	 -3.9498 	 -0.0638
99 	 17.3 	 0.0010 	 -3.9518 	 -0.1863
100 	 17.8 	 0.0010 	 -4.3694 	 -0.3275
101 	 17.6 	 0.0010 	 -4.4928 	 -0.2341
102 	 17.1 	 0.0010 	 -4.5282 	 -0.0965
103 	 17.0 	 0.0010 	 -4.5492 	 -0.0814
104 	 17.0 	 0.0010 	 -4.5585 	 -0.2020
105 	 17.3 	 0.0010 	 -4.5845 	 -0.0378
106 	 17.1 	 0.0010 	 -4.5954 	 -0.1149
107 	 17.1 	 0.0010 	 -4.6083 	 -0.1823
108 	 17.1 	 0.0010 	 -4.6209 	 -0.3326
109 	 17.1 	 0.0010 	 -4.6327 	 -0.0772
110 	 17.0 	 0.0010 	 -4.6440 	 -0.1213
111 	 17.3 	 0.0010 	 -4.6488 	 -0.1478
112 	 17.0 	 0.0010 	 -4.6626 	 -0.0989
113 	 17.1 	 0.0010 	 -4.6655 	 -0.1176
114 	 17.1 	 0.0010 	 -4.6814 	 -0.2846
115 	 17.1 	 0.0010 	 -4.6826 	 -0.0846
116 	 17.4 	 0.0010 	 -4.6892 	 -0.0525
117 	 17.0 	 0.0010 	 -4.6992 	 -0.2398
118 	 17.2 	 0.0010 	 -4.7101 	 -0.3044
119 	 17.1 	 0.0010 	 -4.7150 	 -0.0971
120 	 17.1 	 0.0010 	 -4.7243 	 -0.1510
121 	 17.1 	 0.0010 	 -4.7266 	 -0.1555
122 	 17.3 	 0.0010 	 -4.7363 	 -0.3252
123 	 17.1 	 0.0010 	 -4.7431 	 -0.0851
124 	 17.0 	 0.0010 	 -4.7477 	 -0.1776
125 	 17.0 	 0.0010 	 -4.7512 	 -0.2552
126 	 17.1 	 0.0010 	 -4.7602 	 -0.0709
127 	 17.3 	 0.0010 	 -4.7625 	 -0.0476
128 	 17.2 	 0.0010 	 -4.7652 	 -0.4372
129 	 17.2 	 0.0010 	 -4.7734 	 -0.4897
130 	 17.1 	 0.0010 	 -4.7762 	 -0.1131
131 	 17.1 	 0.0010 	 -4.7837 	 -0.0164
132 	 17.1 	 0.0010 	 -4.7919 	 -0.0916
133 	 17.3 	 0.0010 	 -4.7942 	 -0.1550
134 	 17.0 	 0.0010 	 -4.8047 	 -0.0172
135 	 17.1 	 0.0010 	 -4.8044 	 -0.1582
136 	 17.1 	 0.0010 	 -4.8140 	 -0.1621
137 	 17.1 	 0.0010 	 -4.8208 	 -0.0874
138 	 17.3 	 0.0010 	 -4.8187 	 -0.4322
139 	 17.1 	 0.0010 	 -4.8218 	 -0.1593
140 	 17.2 	 0.0010 	 -4.8326 	 -0.0141
141 	 17.1 	 0.0010 	 -4.8402 	 -0.2325
142 	 17.1 	 0.0010 	 -4.8384 	 -0.0557
143 	 17.1 	 0.0010 	 -4.8475 	 -0.3468
144 	 17.3 	 0.0010 	 -4.8517 	 -0.1598
145 	 17.1 	 0.0010 	 -4.8537 	 -0.0725
146 	 17.2 	 0.0010 	 -4.8573 	 -0.1853
147 	 17.1 	 0.0010 	 -4.8672 	 -0.2080
148 	 17.1 	 0.0010 	 -4.8703 	 -0.0447
149 	 17.1 	 0.0001 	 -4.8670 	 -0.1948
150 	 17.5 	 0.0001 	 -4.9035 	 -0.1137
151 	 17.1 	 0.0001 	 -4.9214 	 -0.0121
152 	 17.1 	 0.0001 	 -4.9265 	 -0.1055
153 	 17.1 	 0.0001 	 -4.9294 	 -0.0566
154 	 17.1 	 0.0001 	 -4.9308 	 -0.0635
155 	 17.3 	 0.0001 	 -4.9347 	 -0.2186
156 	 17.0 	 0.0001 	 -4.9357 	 -0.1629
157 	 17.1 	 0.0001 	 -4.9404 	 -0.0473
158 	 17.1 	 0.0001 	 -4.9424 	 -0.2713
159 	 17.1 	 0.0001 	 -4.9434 	 -0.2158
160 	 17.0 	 0.0001 	 -4.9442 	 -0.1160
161 	 17.3 	 0.0001 	 -4.9466 	 -0.2764
162 	 17.0 	 0.0001 	 -4.9489 	 -0.2781
163 	 17.1 	 0.0001 	 -4.9510 	 -0.1770
164 	 17.0 	 0.0001 	 -4.9519 	 -0.1930
165 	 17.1 	 0.0001 	 -4.9540 	 -0.1703
166 	 17.4 	 0.0001 	 -4.9547 	 -0.0829
167 	 17.0 	 0.0001 	 -4.9559 	 -0.1705
168 	 17.1 	 0.0001 	 -4.9598 	 -0.0346
169 	 17.0 	 0.0001 	 -4.9607 	 -0.0339
170 	 17.1 	 0.0001 	 -4.9603 	 -0.1944
171 	 17.1 	 0.0001 	 -4.9624 	 -0.0871
172 	 17.4 	 0.0001 	 -4.9622 	 -0.1912
173 	 17.1 	 0.0001 	 -4.9635 	 -0.1912
174 	 17.0 	 0.0001 	 -4.9670 	 -0.2843
175 	 17.0 	 0.0001 	 -4.9659 	 -0.0048
176 	 17.2 	 0.0001 	 -4.9679 	 -0.1065
177 	 17.4 	 0.0001 	 -4.9716 	 -0.1891
178 	 17.1 	 0.0001 	 -4.9679 	 -0.2891
179 	 17.1 	 0.0001 	 -4.9721 	 -0.0168
180 	 17.3 	 0.0001 	 -4.9722 	 -0.1811
181 	 17.3 	 0.0001 	 -4.9734 	 -0.1420
182 	 17.3 	 0.0001 	 -4.9749 	 -0.2052
183 	 17.7 	 0.0001 	 -4.9756 	 -0.0380
184 	 16.9 	 0.0001 	 -4.9754 	 -0.0658
185 	 17.1 	 0.0001 	 -4.9765 	 -0.2323
186 	 17.0 	 0.0001 	 -4.9791 	 -0.0536
187 	 17.0 	 0.0001 	 -4.9785 	 -0.1338
188 	 17.4 	 0.0001 	 -4.9796 	 -0.1462
189 	 17.0 	 0.0001 	 -4.9812 	 -0.0570
190 	 17.0 	 0.0001 	 -4.9818 	 -0.2587
191 	 17.1 	 0.0001 	 -4.9839 	 -0.1734
192 	 17.1 	 0.0001 	 -4.9842 	 -0.0608
193 	 17.1 	 0.0001 	 -4.9842 	 -0.3128
194 	 17.3 	 0.0001 	 -4.9845 	 -0.0378
195 	 17.1 	 0.0001 	 -4.9867 	 -0.2280
196 	 17.1 	 0.0001 	 -4.9866 	 -0.0389
197 	 17.1 	 0.0001 	 -4.9876 	 -0.0842
198 	 17.1 	 0.0001 	 -4.9891 	 -0.3508
199 	 17.1 	 0.0001 	 -4.9877 	 -0.2780
Total train time: 62.5548 minutes
Test sample size = 50
Best Epoch 	 Test Loss 	 Test Time
12 	 3.2916 	 7.1927
Last Epoch 	 Test Loss 	 Test Time
199 	 -1.7624 	 7.0607
Test sample size = 100
Best Epoch 	 Test Loss 	 Test Time
12 	 -1.0210 	 7.0766
Last Epoch 	 Test Loss 	 Test Time
199 	 -0.7602 	 7.0460
Test sample size = 250
Best Epoch 	 Test Loss 	 Test Time
12 	 -0.2229 	 7.0745
Last Epoch 	 Test Loss 	 Test Time
199 	 -0.3235 	 7.0916
Test sample size = 500
Best Epoch 	 Test Loss 	 Test Time
12 	 -0.7100 	 7.1710
Last Epoch 	 Test Loss 	 Test Time
199 	 0.1029 	 7.1680
Test sample size = 1000
Best Epoch 	 Test Loss 	 Test Time
12 	 -0.2494 	 7.3181
Last Epoch 	 Test Loss 	 Test Time
199 	 -0.2858 	 7.3133
Test sample size = 5000
Best Epoch 	 Test Loss 	 Test Time
12 	 0.1118 	 8.6737
Last Epoch 	 Test Loss 	 Test Time
199 	 -0.0826 	 8.7017
Test sample size = 8000
Best Epoch 	 Test Loss 	 Test Time
12 	 -0.2005 	 9.6787
Last Epoch 	 Test Loss 	 Test Time
199 	 -0.0381 	 9.6760
Test sample size = 10000
Best Epoch 	 Test Loss 	 Test Time
12 	 0.0330 	 10.5955
Last Epoch 	 Test Loss 	 Test Time
199 	 0.0552 	 10.3734
